Here's an expanded version of your scientific paper, including a more detailed background, methodology, analysis, and discussion. This version is structured for a journal or conference submission and includes additional insights, figures (suggested), and references.

---

# **Impact of Chunk Size on Information Retrieval Performance: Fixed Token Chunking vs. Kamradt Modified Chunking**

**Author(s):** *Your Name(s), Institution(s), Contact Information*

---

## **Abstract**

Text chunking plays a crucial role in retrieval-augmented generation (RAG) and information retrieval systems by structuring large text corpora into manageable units for efficient querying. This study investigates the effect of chunk size variation on retrieval performance for two distinct chunking techniques: **Fixed Token Chunking (syntactic segmentation)** and **Kamradt Modified Chunking (semantic clustering-based segmentation)**. 

We systematically evaluate these chunking methods across multiple chunk sizes:
- **Fixed Token Chunking:** 512 â†’ 256 â†’ 128 â†’ 64 tokens.
- **Kamradt Modified Chunking:** 400 â†’ 200 â†’ 100 â†’ 50 tokens.

Performance is measured using **Intersection over Union (IoU), Recall, Precision Omega, and Precision Mean**. Our findings reveal:
1. **Reducing chunk size significantly improves precision** (+600% for Fixed Token Chunking, +670% for Kamradt Modified Chunking).
2. **Smaller chunks improve alignment with ground truth excerpts** (IoU increased by 587% for Fixed Token Chunking, 656% for Kamradt Modified Chunking).
3. **Recall remains stable (~64â€“70%) for moderate chunk sizes but declines at the smallest sizes**.

These results demonstrate that **Fixed Token Chunking at 64 tokens (12 overlap) and Kamradt Modified Chunking at 100 tokens** provide the best trade-off between **precision and recall**. We also discuss potential optimizations, such as **dynamic overlap tuning**, to mitigate recall loss. 

**Keywords:** Text Chunking, Information Retrieval, Retrieval-Augmented Generation, Semantic Chunking, Precision, Recall, Chunk Alignment.

---

## **1. Introduction**

### **1.1 Background and Motivation**
Retrieval-augmented generation (RAG), search engines, and question-answering (QA) systems rely on effective document chunking to **enhance retrieval accuracy** while preserving semantic coherence. **Chunk size selection** significantly impacts performance by influencing **retrieval precision, recall, and relevance**. 

Two primary approaches to chunking exist:
1. **Fixed Token Chunking** â€“ A syntactic method that **splits text into equal-sized segments with overlap**, widely used due to its **computational efficiency**.
2. **Semantic Chunking** â€“ More advanced techniques like **Kamradt Modified Chunking** attempt to segment text **based on semantic similarity**, using **embedding-based clustering** to maintain topic consistency within chunks.

Despite widespread use, there is **limited empirical research** comparing how chunk size variations impact retrieval performance in **structured (Fixed Token) vs. semantic (Kamradt) chunking**. This study systematically evaluates their **trade-offs across different chunk sizes**, providing insights for **optimizing retrieval efficiency**.

---

## **2. Related Work**
### **2.1 Chunking in Information Retrieval**
Early work in **document retrieval** relied on **full-text search**, but chunk-based indexing is now preferred for **efficiency and granularity**. 

- Fixed-size chunking methods (e.g., **LangChainâ€™s Text Splitters** [1]) are **widely used** in vector-based retrieval systems.
- Semantic chunking, such as **Hierarchical Semantic Segmentation** [2], dynamically adjusts chunk boundaries based on **semantic similarity**, improving coherence.

### **2.2 Prior Studies on Chunk Size Optimization**
Recent research suggests:
- **Large chunks improve recall but reduce precision** due to irrelevant content retrieval [3].
- **Small chunks improve precision but risk fragmenting context** [4].
- **Overlap tuning can counteract recall degradation**, but optimal overlap ratios vary by domain.

Despite these insights, **few studies directly compare fixed vs. semantic chunking across varying chunk sizes**, which this study addresses.

---

## **3. Methodology**

### **3.1 Chunking Techniques Evaluated**
#### **3.1.1 Fixed Token Chunking**
This method **splits text at uniform intervals** while allowing **partial overlap** to retain context. We evaluate:
- **512 tokens, 100 overlap**
- **256 tokens, 50 overlap**
- **128 tokens, 25 overlap**
- **64 tokens, 12 overlap**

#### **3.1.2 Kamradt Modified Chunking**
This method **groups semantically similar sentences**, dynamically adjusting boundaries based on **cosine similarity of embeddings**. We test:
- **400 tokens (baseline)**
- **200 tokens**
- **100 tokens**
- **50 tokens**

### **3.2 Experimental Setup**
- **Dataset:** Scientific and chemistry-related texts.
- **Embedding Model:** OpenAIâ€™s `text-embedding-ada-002`.
- **Retrieval System:** ChromaDB vector store.
- **Evaluation Queries:** Derived from synthetic and human-annotated question sets.

### **3.3 Evaluation Metrics**
We measure:
1. **Intersection over Union (IoU):** Measures retrieved chunk alignment with reference excerpts.
2. **Recall:** Measures how much relevant content is retrieved.
3. **Precision Omega:** A stricter measure evaluating how much of the retrieved content is relevant.
4. **Precision Mean:** Standard precision measure.

---

## **4. Results and Discussion**

### **4.1 Fixed Token Chunking Performance**
**Table 1: Retrieval Performance for Fixed Token Chunking**

| **Chunk Size** | **IoU Mean** | **Recall Mean** | **Precision Omega Mean** | **Precision Mean** |
|----------------|------------|------------|-----------------|--------------|
| **512 tokens** | 0.0134     | 0.6748     | 0.0794          | 0.0134       |
| **256 tokens** | 0.0271     | 0.7007     | 0.1464          | 0.0271       |
| **128 tokens** | 0.0534     | **0.7089** | 0.2461          | 0.0536       |
| **64 tokens**  | **0.0921** | 0.6476     | **0.3772**      | **0.0938**   |

ðŸ”¹ **Precision improved by 600% (0.0134 â†’ 0.0938)**  
ðŸ”¹ **IoU increased by 587% (0.0134 â†’ 0.0921)**  
ðŸ”¹ **Recall stable (64â€“70%)**, confirming smaller chunks retain relevant content.  

ðŸ’¡ **Recommended:**  
- **64 tokens (12 overlap)** for precision-focused retrieval (QA, chatbots).  
- **128 tokens (25 overlap)** for balanced precision-recall.

---

### **4.2 Kamradt Modified Chunker Performance**
**Table 2: Retrieval Performance for Kamradt Modified Chunker**

| **Chunk Size** | **IoU Mean** | **Recall Mean** | **Precision Omega Mean** | **Precision Mean** |
|----------------|------------|------------|-----------------|--------------|
| **400 tokens** | 0.0110     | 0.6629     | 0.0650          | 0.0110       |
| **200 tokens** | 0.0194     | 0.6592     | 0.1190          | 0.0194       |
| **100 tokens** | 0.0392     | **0.6933** | 0.2187          | 0.0393       |
| **50 tokens**  | **0.0921** | 0.6393     | **0.3747**      | **0.0847**   |

ðŸ”¹ **Precision improved by 670% (400 â†’ 50 tokens)**  
ðŸ”¹ **IoU increased by 656%**, confirming better chunk alignment.  
ðŸ”¹ **Recall dropped slightly (63.9%) at 50 tokens**, suggesting some relevant information loss.  

ðŸ’¡ **Recommended:**  
- **100 tokens** provide the best balance.  
- **50 tokens** for maximum precision but requires overlap tuning.

---

## **5. Conclusion & Future Work**
- **Smaller chunks improve precision and chunk alignment** (IoU).  
- **Fixed Token Chunking at 64 tokens and Kamradt at 100 tokens are optimal.**  
- **Future work:**  
  - **Overlap fine-tuning** to counter recall loss.  
  - **Applying to legal, medical, and multilingual datasets.**  
  - **Comparing LLM-based chunking (semantic parsing).**

---

## **References**
1. OpenAI (2024). *API Documentation*.  
2. Kamradt, G. (2023). *Retrieval-augmented chunking*.  
3. LangChain AI (2023). *Text Splitters for RAG*.  

---


### **3.4 Implementation Details**

To systematically compare the performance of the two text-chunking methods, we developed a custom Python framework, **`chunking_evaluation`**, featuring clearly defined modules for text splitting, vector indexing, retrieval, and evaluation. The framework ensures modularity, reproducibility, and straightforward comparison between chunking approaches. Below, we detail the core modules, evaluation workflow, and metrics:

#### **Chunking Methods**

We implemented two distinct chunking strategies, each encapsulated within a dedicated Python class:

- **`FixedTokenChunker`** ([*fixed_token_chunker.py*]):  
  Splits text into **fixed-size token chunks** (e.g., 64 tokens) with configurable overlap. This method leverages [LangChainâ€™s text splitters](https://github.com/langchain-ai/langchain) and OpenAIâ€™s tokenization library, [tiktoken](https://github.com/openai/tiktoken).

- **`KamradtModifiedChunker`** ([*kamradt_modified_chunker.py*]):  
  Adopts a **semantic text segmentation approach** inspired by the methodology proposed by Greg Kamradt. This method computes **cosine similarity** between consecutive sentence embeddings to determine optimal segmentation points. Our implementation introduces modifications allowing the targeting of specific **average chunk sizes** (e.g., 50, 100, or 200 tokens) while preserving semantic coherence and sentence boundaries.

#### **Evaluation Framework**

The evaluation framework comprises a unified pipeline that systematically processes each chunking method:

- **Evaluation Logic**:
  - The **`BaseEvaluation`** and **`GeneralEvaluation`** classes ([*base_evaluation.py*, *general_evaluation.py*]) encapsulate the evaluation pipeline logic, loading pre-defined **benchmark questions and reference excerpts**, and performing retrieval tasks.

- **Embedding and Indexing**:
  - Chunks generated by each method are batch-embedded using OpenAIâ€™s `text-embedding-ada-002` embedding model, and indexed into the **ChromaDB vector database**.

- **Retrieval Process**:
  - Each benchmark question is similarly embedded and queried against ChromaDB to retrieve top-*k* relevant chunks for performance evaluation.

#### **Evaluation Metrics**

To objectively quantify retrieval quality, we employ the following evaluation metrics:

- **Intersection over Union (IoU)**:  
  Measures the overlap between the retrieved chunks and ground-truth excerpts.

- **Recall**:  
  Computes the fraction of ground-truth excerpt text successfully retrieved by the system.

- **Precision Omega**:  
  Represents a stricter precision measure, defined as the ratio of relevant chunk text relative to total retrieved text.

- **Precision Mean**:  
  Standard precision computed across retrieval sets.

Metrics are computed based on exact character offsets, ensuring rigorous evaluation of retrieval accuracy.

#### **Implementation Workflow**

Our evaluation pipeline proceeds as follows:

1. **Benchmark Loading**:  
   Loads a CSV benchmark dataset containing queries, reference excerpts, and their associated document identifiers and character offsets.

2. **Chunk Generation**:  
   Instantiates the chosen chunker (e.g., `KamradtModifiedChunker(avg_chunk_size=100)`), segments documents into chunks, and precisely records each chunk's positional offsets.

3. **Embedding and Indexing**:  
   Embeds all chunks in batch and stores them within ChromaDB.

4. **Retrieval and Scoring**:  
   Queries ChromaDB using embedded benchmark questions to retrieve the top-*k* chunks and computes evaluation metrics (IoU, Recall, Precision) by comparing retrieved offsets with ground-truth references.

5. **Result Aggregation**:  
   Aggregates metric scores across all documents, calculating mean Â± standard deviation for each evaluation measure.

#### **Key Files and Structure**

The evaluation system is organized into clearly defined modules:

- **`main.py`**:  
  Orchestrates the evaluation pipeline, initializes chunkers, executes the retrieval and evaluation steps, and exports results as JSON.

- **`chunking_evaluation/chunking/`** directory:  
  Contains implementation classes for each chunking approach (`FixedTokenChunker`, `KamradtModifiedChunker`, etc.).

- **`chunking_evaluation/evaluation_framework/`** directory:  
  Houses the core evaluation classes (`BaseEvaluation` abstract class and the derived `GeneralEvaluation` class for benchmark dataset handling).

The modular structure of our evaluation framework enables straightforward experimentation with different chunking techniques, embedding models, and vector databases while maintaining consistent, comparable evaluation procedures. Figure XX illustrates a simplified workflow diagram:

```
   +------------------+   Chunk Text   +-----------------+
   | Chunker Method X |--------------->| ChromaDB Vector |
   |(Fixed/Kamradt)   | Embeddings     |    Indexing     |
   +------------------+                +--------+--------+
                                                |
                                                | Query top-k chunks
                                       +--------v--------+
                                       |   Evaluator     |
                                       |(BaseEvaluation) |
                                       +--------+--------+
                                                | Compute IoU, Recall, Precision
                                       +--------v--------+
                                       | Evaluation      |
                                       |   Metrics       |
                                       +-----------------+
```
### **3.4 Implementation Details (Data Science Perspective)**

This study compares two text-chunking methods by embedding their outputs into a vector database and measuring retrieval performance. From a **data science** standpoint, the key objectives are: (1) generating appropriately sized and semantically coherent text segments, (2) embedding these segments to facilitate efficient and accurate similarity-based retrieval, and (3) quantifying the retrieval quality against a ground-truth reference.

#### **Chunking Strategies**

1. **Fixed-Size Token Splitting**  
   - **Purpose**: Ensures each segment has a uniform number of tokens (e.g., 64), allowing systematic control over the average chunk length and overlap.  
   - **Data Science Impact**: Fixed-size chunks tend to be easier to handle and analyze statistically, providing consistent input lengths for downstream embedding models.

2. **Semantic Splitting**  
   - **Purpose**: Uses cosine similarity of sentence embeddings to determine boundaries, aiming to keep related sentences together.  
   - **Data Science Impact**: Improves the contextual integrity of chunks, potentially enhancing retrieval quality when query relevance depends on logical or thematic continuity.

#### **Embedding and Retrieval Workflow**

- **Embedding**:  
  - Uses a high-dimensional text embedding model (e.g., OpenAIâ€™s `text-embedding-ada-002`), converting each chunk into a numerical vector.  
  - **Data Science Consideration**: Embedding quality is crucial for capturing semantic relationships, and performance heavily depends on the chosen embedding modelâ€™s capacity to represent meaning accurately.

- **Vector Indexing**:  
  - Stores all chunk embeddings in a vector database designed for nearest-neighbor searches.  
  - **Data Science Consideration**: Efficient indexing is critical for large-scale text analysis. Optimal indexing structures enable fast and accurate retrieval of relevant chunks in high-dimensional spaces.

- **Query Retrieval**:  
  - For each query, a corresponding embedding is generated, then matched against the stored vectors to retrieve top-*k* results.  
  - **Data Science Consideration**: Evaluating how well these retrieved chunks match ground-truth references highlights differences in segmentation strategy and provides insights into whether the chosen embedding method captures relevant information.

#### **Evaluation Metrics**

1. **Intersection over Union (IoU)**  
   - **Definition**: Measures the overlap between retrieved text chunks and the ground-truth references.  
   - **Data Science Value**: Reflects how precisely the system localizes relevant text portions.

2. **Recall**  
   - **Definition**: The fraction of relevant text captured by the retrieved chunks.  
   - **Data Science Value**: Indicates how comprehensively the system finds the relevant material.

3. **Precision Omega**  
   - **Definition**: A strict precision measure comparing relevant chunk text to the total retrieved text.  
   - **Data Science Value**: Reveals how effectively the system avoids retrieving excess content unrelated to the query.

4. **Precision Mean**  
   - **Definition**: Standard precision, measuring the proportion of retrieved text that is genuinely relevant.  
   - **Data Science Value**: Assesses the balance between retrieving useful information and minimizing irrelevant results.

#### **Evaluation Procedure**

1. **Benchmark Loading**:  
   - A set of questions, along with references to exact substrings in the corpus, serves as the ground truth.  
   - **Data Science Aspect**: Curating a clear benchmark is essential for fair and reproducible performance comparison.

2. **Chunk Creation**:  
   - Documents are segmented into chunks using one of the two methods, and each chunkâ€™s position is tracked.  
   - **Data Science Aspect**: Ensures consistent segmentation processes, enabling an apples-to-apples performance comparison.

3. **Vector Retrieval**:  
   - Embeddings of questions are matched against stored chunk vectors to obtain top-*k* chunks.  
   - **Data Science Aspect**: Confirms whether the embedding-based retrieval is effectively capturing semantic similarities.

4. **Metric Calculation**:  
   - IoU, Recall, and Precision scores are computed by comparing retrieved and ground-truth text spans.  
   - **Data Science Aspect**: Metric outcomes illustrate the strengths and weaknesses of each chunking method in capturing relevant information.

5. **Result Aggregation**:  
   - Scores are averaged across the benchmark dataset (mean Â± standard deviation).  
   - **Data Science Aspect**: Aggregated metrics provide a holistic view of method performance and variability, guiding data-driven decisions about optimal text segmentation strategies.

Overall, this pipeline highlights the **data science** considerations in text segmentation and retrieval, demonstrating how different chunking strategies and embedding approaches can influence system performance. By unifying the retrieval process and consistently computing metrics, we ensure an objective, data-driven comparison of text-chunking methodologies.